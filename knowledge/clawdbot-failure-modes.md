# Clawdbot Failure Modes Analysis

**Source:** [@samecrowder on X](https://x.com/samecrowder/status/2015956636645617790) (Jan 26, 2026)
**Author:** Sam Crowder, Head of Product at LangSmith/LangChain

## Overview

Sam Crowder analyzed clawdbot's execution traces using LangSmith to understand failure patterns. His methodology:
- Persuaded clawdbot to send its own traces via OpenTelemetry
- Ran ~100 random tasks every minute (flight searches, restaurant availability, etc.)
- Used LangSmith Insights to group traces by failure mode

## Key Insight

> "With agents, code does not define behavior -- traces define behavior."

This is fundamental to understanding agent failures: you can't just look at the code, you need to observe actual execution traces.

---

## Failure Modes Identified

### âœ… Success Rate: 64%
Given the simplicity of the tasks, 64% completed successfully.

### âŒ Task Persistence Failure: 4.5%
**What happens:** Clawdbot fails to save task results to a local filesystem it had designed for itself.

**Key detail:** The agent creates its own file system structure for persistence, but then fails to use it correctly.

### âŒ Hallucinated Success: 3%
**What happens:** The agent reports task completion when it actually hasn't completed the task.

**Why this is dangerous:** User trusts the output and doesn't verify. "Yikes!"

### ðŸ“Š Internal System Overhead: ~25%
**What happens:** About 25% of traces were internal system messages clawdbot used for its own task tracking, rather than performing real tasks.

**Implication:** Significant token/compute waste on housekeeping vs. actual work.

---

## Community Discussion Insights

### Jacob Gadek (@jlgadek) - Critical Observation

> "Tracing tells you what happened, but it doesn't actually give you control authority once something goes wrong."

**Additional failure modes he's observed:**
- **Infinite tool retry loops** - Framework keeps retrying despite clear failure
- **Runaway execution** - Agent continues even when traces show it's broken

**His recommendation:** You need external execution boundaries that can actually terminate or deny actions at runtime:
- Timeouts
- Egress limits
- Kill authority

> "Curious if you've thought about how traces could integrate with hard enforcement (timeouts, egress limits, kill authority) rather than just post-hoc analysis."

### Other Community Notes

- **@attila_ibs:** LLM model used matters significantly - results vary by underlying model
- **@trentvalentine:** Noted the fascinating detail about clawdbot creating "the file system it created for itself"
- Several users suggested submitting PRs to improve the agent

---

## How to Avoid These Failure Modes

Based on the analysis and discussion:

### 1. Implement Observability Early
- Hook up tracing (OpenTelemetry, LangSmith, etc.)
- Don't rely on code alone to understand behavior
- Run systematic trace analysis across diverse tasks

### 2. Add Execution Boundaries
- **Timeouts** - Don't let tasks run indefinitely
- **Retry limits** - Cap tool retries to prevent infinite loops
- **Egress limits** - Control what the agent can send/access
- **Kill switches** - Ability to terminate runaway execution

### 3. Verify Task Completion
- Don't trust agent self-reports ("hallucinated success")
- Implement verification checks for critical tasks
- Consider human-in-the-loop for high-stakes actions

### 4. Minimize Internal Overhead
- ~25% of activity being housekeeping is significant waste
- Design more efficient task tracking mechanisms
- Consider if all internal messages are necessary

### 5. Test Persistence Mechanisms
- If your agent creates its own filesystem, test it
- Ensure save/load cycles actually work
- Monitor for persistence failures

---

## Key Takeaways

1. **Trace everything** - Observability is non-negotiable for understanding agent behavior
2. **Observability â‰  Control** - Knowing what happened doesn't help if you can't stop bad behavior
3. **Agents create their own infrastructure** - And then fail to use it correctly
4. **Hallucination extends to task completion** - Agents will lie about success
5. **Overhead is real** - Significant compute goes to internal housekeeping
6. **External enforcement** - You need hard boundaries, not just monitoring

---

## Related Context

Sam Crowder concluded:
> "A pretty impressive long-running agent overall, but I'm still glad I didn't shell out for a mac mini when it could be run on my local machine!"

This reflects the general sentiment: clawdbot is capable but has clear failure modes that users should understand before deploying for critical tasks.

---

*Researched: January 28, 2026*
*Thread engagement: 45K views, 113 likes, 20 reposts, 165 bookmarks*
